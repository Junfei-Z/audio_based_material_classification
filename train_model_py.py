# -*- coding: utf-8 -*-
"""train_model.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hlWasTLJ3K4NWWYGv954jPR4-pju_IhF
"""

#import necessary libaries
import os
import zipfile
import numpy as np
import librosa
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from google.colab import drive

#unzip the traning data and evluation data
def unzip(file_path):
  """
  Unzips a zip file to the current working directory.

  Args:
    file_path: Path to the zip file.
  """
  with zipfile.ZipFile(file_path, 'r') as zip_ref:
    zip_ref.extractall('.')

unzip('/content/wav_cut2.zip') # Call the unzip function with the file path
unzip('/content/Sample_Evaluation_Data.zip')

# Define label mappings
CLASS_TO_LABEL = {
    'water': 0,
    'table': 1,
    'sofa': 2,
    'railing': 3,
    'glass': 4,
    'blackboard': 5,
    'ben': 6,
}
LABEL_TO_CLASS = {label: class_name for class_name, label in CLASS_TO_LABEL.items()}

def load_data(base_dir, class_to_label):
    """
    Loads WAV file paths and their corresponding labels from the specified directory.

    Args:
        base_dir (str): The base directory containing class subfolders.
        class_to_label (dict): A mapping from class names to numerical labels.

    Returns:
        tuple: Two numpy arrays containing file paths (X) and labels (Y).
    """
    X, Y = [], []
    for class_name, label in class_to_label.items():
        class_folder = os.path.join(base_dir, class_name)
        if os.path.isdir(class_folder):
            # List all .wav files in the class folder
            wav_files = [
                os.path.join(class_folder, file)
                for file in os.listdir(class_folder)
                if file.endswith('.wav')
            ]
            X.extend(wav_files)
            Y.extend([label] * len(wav_files))
    return np.array(X), np.array(Y)

# Define directories
train_dir = '/content/wav_cut2'
eval_dir = '/content/Sample_Evaluation_Data'

# Load training data
X, Y = load_data(train_dir, CLASS_TO_LABEL)
print(f"Training data: {X.shape}, Labels: {Y.shape}")

# Load evaluation data
X_eval, Y_eval = load_data(eval_dir, CLASS_TO_LABEL)
print(f"Evaluation data: {X_eval.shape}, Labels: {Y_eval.shape}")

# Feature extraction from audio files
def extract_features(file_path):
    try:
        y, sr = librosa.load(file_path, sr=None)
        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=22)  # Extract 22 MFCC features
        return np.mean(mfcc.T, axis=0)  # Take mean over time
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None
# Extract features for all files
X_features = [extract_features(file) for file in X]
X_features = np.array([feat for feat in X_features if feat is not None])
scaler = StandardScaler()
X_features = scaler.fit_transform(X_features)
Y = Y[:len(X_features)]  # Ensure labels align with features
print(f"Features shape: {X_features.shape}, Labels shape: {Y.shape}")

Xeval_features = [extract_features(file) for file in X_eval]
Xeval_features = np.array([feat for feat in Xeval_features if feat is not None])
scaler = StandardScaler()
Xeval_features = scaler.fit_transform(Xeval_features)
Y_eval = Y_eval[:len(Xeval_features)]  # Ensure labels align with features
print(f"Features shape: {Xeval_features.shape}, Labels shape: {Y_eval.shape}")

#Fourier Transformation to extrac features
def extract_features_fft(file_path, n_fft=2048, hop_length=512, n_features=30):
    """
    Extracts Fourier Transform-based features from an audio file.

    Parameters:
    - file_path (str): Path to the audio file.
    - n_fft (int): Number of FFT components.
    - hop_length (int): Number of samples between successive frames.
    - n_features (int): Number of frequency bins to retain as features.

    Returns:
    - np.ndarray or None: Extracted feature vector or None if an error occurs.
    """
    try:
        # Load the audio file
        y, sr = librosa.load(file_path, sr=None)

        # Compute the Short-Time Fourier Transform (STFT)
        stft = np.abs(librosa.stft(y, n_fft=n_fft, hop_length=hop_length))

        # Compute the mean magnitude for each frequency bin across all frames
        mean_spectrum = np.mean(stft, axis=1)

        # If the number of frequency bins is less than desired, pad with zeros
        if len(mean_spectrum) < n_features:
            mean_spectrum = np.pad(mean_spectrum, (0, n_features - len(mean_spectrum)), 'constant')

        # Select the first 'n_features' frequency bins as the feature vector
        feature_vector = mean_spectrum[:n_features]

        return feature_vector
    except Exception as e:
        print(f"Error processing {file_path}: {e}")
        return None

# Example usage for training data
def prepare_features(X, Y, extract_func):
    """
    Extracts and scales features for a dataset.

    Parameters:
    - X (list): List of file paths.
    - Y (np.ndarray): Array of labels.
    - extract_func (function): Feature extraction function.

    Returns:
    - tuple: Scaled features and aligned labels.
    """
    # Extract features for all files
    features = [extract_func(file) for file in X]

    # Filter out None values (failed extractions)
    features = [feat for feat in features if feat is not None]

    # Convert to NumPy array
    features = np.array(features)

    # Scale the features
    scaler = StandardScaler()
    scaled_features = scaler.fit_transform(features)

    # Align labels with the extracted features
    aligned_labels = Y[:len(scaled_features)]

    print(f"Features shape: {scaled_features.shape}, Labels shape: {aligned_labels.shape}")

    return scaled_features, aligned_labels

# Example usage for evaluation data
def prepare_evaluation_features(X_eval, Y_eval, extract_func):
    """
    Extracts and scales features for evaluation dataset.

    Parameters:
    - X_eval (list): List of evaluation file paths.
    - Y_eval (np.ndarray): Array of evaluation labels.
    - extract_func (function): Feature extraction function.

    Returns:
    - tuple: Scaled evaluation features and aligned labels.
    """
    # Extract features for all evaluation files
    eval_features = [extract_func(file) for file in X_eval]

    # Filter out None values (failed extractions)
    eval_features = [feat for feat in eval_features if feat is not None]

    # Convert to NumPy array
    eval_features = np.array(eval_features)

    # Scale the features using the same scaler as training
    scaler = StandardScaler()
    scaled_eval_features = scaler.fit_transform(eval_features)

    # Align labels with the extracted features
    aligned_eval_labels = Y_eval[:len(scaled_eval_features)]

    print(f"Features shape: {scaled_eval_features.shape}, Labels shape: {aligned_eval_labels.shape}")

    return scaled_eval_features, aligned_eval_labels

# Prepare training features and labels
X_features2, Y2 = prepare_features(X, Y, extract_features_fft)

# Prepare evaluation features and labels
Xeval_features2, Y_eval2 = prepare_evaluation_features(X_eval, Y_eval, extract_features_fft)

#model_traning-logistic regression-mfcc features
# Train a Logistic Regression Model on the entire dataset
model = LogisticRegression(max_iter=1000, random_state=42)
model.fit(X_features, Y)

# Save the model weights to Google Drive
weights_path = "/content/model_weights.npy"
bias_path = "/content/model_bias.npy"


np.save(weights_path, model.coef_)
np.save(bias_path, model.intercept_)

print("Model trained on the entire dataset and weights saved.")

# Deployment: Using the Model
def run_trained_model(X_paths):
    # Load the saved weights from Google Drive
    weights = np.load(weights_path)
    bias = np.load(bias_path)

    # Extract features from provided file paths
    features = np.array([extract_features(file) for file in X_paths])

    # Apply the logistic regression model manually
    logits = np.dot(features, weights.T) + bias
    predictions = np.argmax(logits, axis=1)

    return predictions

# Example: Predict on the same training data
Y_pred = run_trained_model(X_eval)
accuracy = accuracy_score(Y_eval, Y_pred)
print(f"Model Accuracy on the entire dataset: {accuracy:.2f}")
print(f"{'File':<50} {'True Label':<15} {'Predicted Label':<15}")
print("-" * 80)
for i in range(len(Xeval_features)):
    # Use X_eval[i] to get the file path instead of Y_eval[i]
    true_label = LABEL_TO_CLASS[Y_eval[i]]  # Corrected to use Y_eval
    predicted_label = LABEL_TO_CLASS[Y_pred[i]]
    print(f"{os.path.basename(X_eval[i]):<50} {true_label:<15} {predicted_label:<15}")

# Define paths to load pre-saved weights and biases
weights_path = "/content/model_weights.npy"
bias_path = "/content/model_bias.npy"

weights_black_path = "/content/model_weights_3.npy"
bias_black_path = "/content/model_bias_3.npy"

# Ensure that the weight and bias files exist
if not (os.path.exists(weights_path) and os.path.exists(bias_path) and
        os.path.exists(weights_black_path) and os.path.exists(bias_black_path)):
    raise FileNotFoundError("One or more model weight/bias files are missing. Please ensure they are saved correctly.")


# Deployment: Using Both Models
def run_trained_model(X_paths):
    # Load the Original model's weights and bias
    original_weights = np.load(weights_path)  # Shape: (1, n_features) for binary or (n_classes, n_features) for multiclass
    original_bias = np.load(bias_path)        # Shape: (1,) or (n_classes,)

    # Load the Blackboard model's weights and bias
    black_weights = np.load(weights_black_path)  # Shape: (1, n_features)
    black_bias = np.load(bias_black_path)        # Shape: (1,)

    X_test_features = np.load('/content/X_test_features.npy')


    # Extract features from provided file paths
    features = np.array([extract_features(file) for file in X_paths])  # Shape: (n_samples, n_features)
    scaler = StandardScaler()
    features_1 = scaler.fit_transform(features)
    # Predict using the Blackboard model
    # Blackboard is a binary classifier: 1 for 'blackboard', 0 otherwise
    black_logits = np.dot(X_test_features, black_weights.T) + black_bias  # Shape: (n_samples, 1)
    black_pred = np.argmax(black_logits, axis=1)
    # print(black_logits)
    # print(X_test_features.shape)
    # print(black_weights.shape)
    # print(black_bias.shape)

    # Predict using the Original model
    original_logits = np.dot(features, original_weights.T) + original_bias  # Shape: (n_samples, n_classes)
    original_pred = np.argmax(original_logits, axis=1)                    # Shape: (n_samples,)
    # print(original_logits)
    # print(features.shape)
    # print(original_weights.shape)
    # print(original_bias.shape)

    # print(f'the test feature is {X_test_features}')
    # print(f'the now feature is {features_1}')

    for i in range(len(black_pred)):
        if black_pred[i] != original_pred[i]:
          print(f'the difference is {i}, the original pre is {original_pred[i]}, the black pre is {black_pred[i]}')
    # Combine predictions:
    # If Blackboard model predicts 'blackboard' (1), set prediction to 5
    # Else, use the Original model's prediction
    final_pred = np.zeros_like(original_pred)
    for i in range(len(black_pred)):
        if black_pred[i] == 5:
            final_pred[i] = 5
            print("the balck model is ",i)
        else:
            final_pred[i] = original_pred[i]

    return final_pred

# Example Usage
def evaluate_model(X_eval, Y_eval):
    # Run the trained models to get predictions
    Y_pred = run_trained_model(X_eval)

    # Calculate accuracy
    accuracy = accuracy_score(Y_eval, Y_pred)
    print(f"Model Accuracy on the evaluation dataset: {accuracy:.2f}")

    # Display detailed prediction results
    print(f"{'File':<50} {'True Label':<15} {'Predicted Label':<15}")
    print("-" * 80)
    for i in range(len(X_eval)):
        file_name = os.path.basename(X_eval[i])
        true_label = LABEL_TO_CLASS.get(Y_eval[i], "Unknown")
        predicted_label = LABEL_TO_CLASS.get(Y_pred[i], "Unknown")
        print(f"{file_name:<50} {true_label:<15} {predicted_label:<15}")

evaluate_model(X_eval, Y_eval)

#model_traning-logistic regression-Fourier Transformation
# Train a Logistic Regression Model on the entire dataset
model2 = LogisticRegression(max_iter=1000, random_state=42)
model2.fit(X_features2, Y2)

# Save the model weights to Google Drive
weights_path2 = "/content/model_weights2.npy" # Correct variable name
bias_path2 = "/content/model_bias2.npy" # Correct variable name

# Save the weights using correct variable names
np.save(weights_path2, model2.coef_) # Changed to weights_path2
np.save(bias_path2, model2.intercept_) # Changed to bias_path2

print("Model trained on the entire dataset and weights saved.")

# Deployment: Using the Model
def run_trained_model(X_paths):
    # Load the saved weights from Google Drive
    weights = np.load(weights_path2)
    bias = np.load(bias_path2)

    # Extract features from provided file paths
    features = np.array([extract_features_fft(file) for file in X_paths])

    # Apply the logistic regression model manually
    logits = np.dot(features, weights.T) + bias
    predictions = np.argmax(logits, axis=1)

    return predictions

# Example: Predict on the same training data
Y_pred = run_trained_model(X_eval)
accuracy = accuracy_score(Y_eval, Y_pred)
print(f"Model Accuracy on the entire dataset: {accuracy:.2f}")
print(f"{'File':<50} {'True Label':<15} {'Predicted Label':<15}")
print("-" * 80)
for i in range(len(Xeval_features)):
    # Use X_eval[i] to get the file path instead of Y_eval[i]
    true_label = LABEL_TO_CLASS[Y_eval[i]]  # Corrected to use Y_eval
    predicted_label = LABEL_TO_CLASS[Y_pred[i]]
    print(f"{os.path.basename(X_eval[i]):<50} {true_label:<15} {predicted_label:<15}")